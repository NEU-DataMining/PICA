# -*- coding: utf-8 -*-
# @Author      : é«˜æ­£æ°
# @File        : pica_app.py
# @Email       : gaozhengj@foxmail.com
# @Date        : 2023/7/22 15:14
# @Description :


from transformers import AutoTokenizer, AutoModel, AutoConfig
import os
import torch
import streamlit as st
from streamlit_chat import message
import json
import re

st.set_page_config(
    page_title="PICA-V1æ¨¡å‹",
    page_icon="ğŸ‘©â€ğŸ«",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'About': """
        -   ç‰ˆæœ¬ï¼šPICA-V1æ¨¡å‹
        -   æ¨¡å‹å¼€å‘è€…ï¼šä¸œåŒ—å¤§å­¦æ•°æ®æŒ–æ˜å®éªŒå®¤
        -   æœ¬ç•Œé¢å¼€å‘è€…ï¼šé«˜æ­£æ°
        """
    }
)
st.header("PICA-V1æ¨¡å‹")
with st.expander("â„¹ï¸ - å…³äºæˆ‘ä»¬", expanded=False):
    st.write(
        """
        -   ç‰ˆæœ¬ï¼šPICA-V1æ¨¡å‹
        -   æ¨¡å‹å¼€å‘è€…ï¼šä¸œåŒ—å¤§å­¦æ•°æ®æŒ–æ˜å®éªŒå®¤
        -   æœ¬ç•Œé¢å¼€å‘è€…ï¼šé«˜æ­£æ°
        """)


def answer(user_history, bot_history, sample=True, top_p=0.75, temperature=0.95):
    """

    :param user_history: ç”¨æˆ·è¾“å…¥çš„å†å²æ–‡æœ¬
    :param bot_history: æœºå™¨ç”Ÿæˆçš„å†å²æ–‡æœ¬
    :param sample: æ˜¯å¦æŠ½æ ·ã€‚ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥è®¾ç½®ä¸ºTrue;
    :param top_p: 0-1ä¹‹é—´ï¼Œç”Ÿæˆçš„å†…å®¹è¶Šå¤šæ ·
    :param temperature:
    :return:
    """
    if len(bot_history) > 0:
        dialog_turn = 5  # è®¾ç½®å†å²å¯¹è¯è½®æ•°
        if len(bot_history) > dialog_turn:
            bot_history = bot_history[-dialog_turn:]
            user_history = user_history[-(dialog_turn + 1):]

        context = "\n".join(
            [f"[Round {i + 1}]\n\né—®ï¼š{user_history[i]}\n\nç­”ï¼š{bot_history[i]}" for i in range(len(bot_history))])
        input_text = context + f"[Round {len(bot_history) + 1}]\n\né—®ï¼š" + user_history[-1] + "\n\nç­”ï¼š"
    else:
        input_text = "[Round 1]\n\né—®ï¼š" + user_history[-1] + "\n\nç­”ï¼š"

    print(input_text)
    if not sample:
        response, history = model.chat(tokenizer, query=input_text, history=None, max_length=2048, num_beams=1,
                                       do_sample=False, top_p=top_p, temperature=temperature, logits_processor=None)
    else:
        response, history = model.chat(tokenizer, query=input_text, history=None, max_length=2048, num_beams=1,
                                       do_sample=True, top_p=top_p, temperature=temperature, logits_processor=None)

    print("æ¨¡å‹åŸå§‹è¾“å‡ºï¼š\n", response)
    # è§„åˆ™æ ¡éªŒï¼Œè¿™é‡Œå¯ä»¥å¢åŠ æ ¡éªŒæ–‡æœ¬çš„è§„åˆ™
    response = re.sub("\n+", "\n", response)
    print('ç­”: ' + response)
    return response


@st.cache_resource
def load_model():
    config = AutoConfig.from_pretrained("/hy-tmp/chatglm2-6b", trust_remote_code=True, pre_seq_len=128)
    model = AutoModel.from_pretrained("/hy-tmp/chatglm2-6b", config=config, trust_remote_code=True).half().quantize(4)
    CHECKPOINT_PATH = '/hy-tmp/PICA-V1'
    prefix_state_dict = torch.load(os.path.join(CHECKPOINT_PATH, "pytorch_model.bin"))
    new_prefix_state_dict = {}
    for k, v in prefix_state_dict.items():
        if k.startswith("transformer.prefix_encoder."):
            new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
    model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)
    model.to(device)
    print('Model Load done!')
    return model


@st.cache_resource
def load_tokenizer():
    tokenizer = AutoTokenizer.from_pretrained("/hy-tmp/chatglm2-6b", trust_remote_code=True)
    print('Tokenizer Load done!')
    return tokenizer


def get_text():
    user_input = st.session_state.user_input
    if 'id' not in st.session_state:
        if not os.path.exists("./history"):
            # åˆ›å»ºä¿å­˜ç”¨æˆ·èŠå¤©è®°å½•çš„ç›®å½•
            os.makedirs("./history")
        json_files = os.listdir("./history")
        id = len(json_files)
        st.session_state['id'] = id

    if user_input:
        st.session_state["past"].append(user_input)
        output = answer(st.session_state.past, st.session_state.generated)
        try:
            st.session_state.generated.append(output)
        except KeyError as e:
            print("Asd")
        # å°†å¯¹è¯å†å²ä¿å­˜æˆjsonæ–‡ä»¶
        dialog_history = {
            'user': st.session_state.past,
            'bot': st.session_state.generated
        }
        with open(os.path.join("./history", str(st.session_state['id']) + '.json'), "w", encoding="utf-8") as f:
            json.dump(dialog_history, f, indent=4, ensure_ascii=False)

    if st.session_state.generated:
        for i in range(len(st.session_state.generated)):
            # æ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
            message(st.session_state.past[i], is_user=True, key=str(i) + '_user', avatar_style="avataaars", seed=26)
            # æ˜¾ç¤ºæœºå™¨äººçš„å›å¤
            message(st.session_state.generated[i], is_user=False, key=str(i), avatar_style="avataaars", seed=5)
    st.session_state["user_input"] = ""


if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = load_model()
    tokenizer = load_tokenizer()

    if 'generated' not in st.session_state:
        st.session_state.generated = []

    if 'past' not in st.session_state:
        st.session_state.past = []

    with st.container():
        st.text_area(label="è¯·åœ¨ä¸‹åˆ—æ–‡æœ¬æ¡†è¾“å…¥æ‚¨çš„å’¨è¯¢å†…å®¹ï¼š", value="",
                     placeholder="è¯·è¾“å…¥æ‚¨çš„æ±‚åŠ©å†…å®¹ï¼Œå¹¶ä¸”ç‚¹å‡»Ctrl+Enterå‘é€ä¿¡æ¯", key="user_input", on_change=get_text)

    if st.button("æ¸…ç†å¯¹è¯ç¼“å­˜"):
        st.session_state.generated = []
        st.session_state.past = []
